# 网页数据爬虫基础

数据爬虫的本质上是文本/流媒体信息获取, 也就是只要是网站加载到页面中的数据, 通过 JavaScript 就可以获取. 

如果只是一些简单的数据获取需求, 其实也可以不通过 **selenium** 这些自动化控制浏览器工具去操作. 只要网站不做 `isTrust` 行为判断, 使用网页脚本也是可行的. 

在本篇文章中, 我们将会介绍网页中的**文本信息**的**获取**以及**下载**. 

## 控制点

制作一个数据爬虫首先需要两个控制点, **开始爬虫控制点**和**爬虫结束条件控制点**. 使用网页脚本制作数据爬虫需要的**开始控制点**一般使用脚本的菜单按钮控制 (`GM_registerMenuCommand`), **结束控制点** 就看具体的爬虫使用场景了. 

所以我们先来制作一个简单的**开始控制点**模板. 

```ts
// @grant        GM_registerMenuCommand
```

```ts
const handleStartDataCollect = () => {
    /* 具体操作逻辑 */
};
GM_registerMenuCommand( '开始爬取数据', handleStartDataCollect );
```

## 爬取数据前的注意事项



爬取数据首先肯定要网站数据加载完成, 但是每个网站的数据加载情况都不一样: 网站是动态加载的, 内容加载模式更是多种多样, 列表加载, 虚拟列表, 无限滚动...... 所以需要依据具体情况去考虑脚本的制作. 

等待数据加载的核心思路是**等待元素加载**, 比如 A 网站的数据是列表加载的, 每一页的数据都是固定的 20 条, 那么每次切换页面之后, 去监听该列表容器的子元素数量, 达到 20 条之后开始爬取. B 网站的数据是无限滚动的, 还使用了懒加载, 那么就要模拟滚动页面之后, 去监听所有的数据懒加载状态有没有消失 (通常会在类名中体现)......

在本篇文章我们将讲述最简单的爬虫场景, 即爬取固定的, 已经加载到页面中的数据. 

这种数据爬取是最简单的, 因为不需要考虑数据加载情况, 不需要手动去控制页面加载. 当开始控制点触发 (用户点击爬取按钮) 的时候, 页面中存在什么数据爬取什么数据就可以了. 



## 爬取 Bilibili 每周必看

> 以 [Bilibili 每周必看](https://www.bilibili.com/v/popular/weekly) 举例: 

下面是 [第一期](https://www.bilibili.com/v/popular/weekly?num=1) 的第一个视频, 可以观察到有以下信息: 

- `title`: 标题 - *这集vlog我们拍了十年，致最美好的青春*
- `coverUrl`: 封面图片 - *爬取封面链接, 本篇文章不涉及图片获取*
- `up`: UP 主 - *AresserA木恩木*
- `views`: 播放量 - *529.2万*
- `danmakus`: 弹幕数 - *6.9万*
- `recommended`: 推荐语 - *暴风流泪推荐！今天也是为别人的神仙爱情流泪的一天！*
- `videoUrl`: 视频链接 - *没有显示在页面中, 跳转页面的 a 元素可以获取到*

![image-20250529235444237](./iamges_04_%E6%95%B0%E6%8D%AE%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/image-20250529235444237.png)

打开开发者工具查看对应的类属性: 

```ts
// 列表容器选择器
const containerSelector = '.video-list';
// 每一个视频信息卡片的选择器
const videoCardSelector = '.video-card';
// 视频信息选择器
const videoInfoSelectorMapper = {
	title: '.video-name',
	up: '.up-name__text',
	videoUrl: '.video-card__content > a',
	coverUrl: '.cover-picture__image',
	recommended: '.weekly-hint',
	views: '.video-stat > .play-text',
	danmakus: '.video-stat > .like-text',
};
```

接下来的思路就是: 

```mermaid
graph LR
	S((开始控制点)) --> 获取列表容器元素 --> 获取每一个视频信息卡片元素 --> T[遍历提取每一个视频卡片中的信息] --> 下载信息 --> E((结束))
```

**获取列表容器元素**

```ts
// 列表容器元素
const container = document.querySelector<HTMLElement>( containerSelector );
if ( !container ) return;
```

**获取视频信息卡片元素列表**

> 这里通过 `Array.from()` , 将 `NodeListOf<HTMLElement>` 类型转为 `Array<HTMLElement>` 类型, 因为后续需要使用到数组方法. 

```ts
// 视频信息卡片元素列表
const videoCardList = Array.from<HTMLElement>( container.querySelectorAll( videoCardSelector ) );
```

**处理单独的视频卡片信息**

> 视频卡片信息处理的逻辑如下: 
>
> - 如果是视频链接 `videoUrl`, 那么是一个 `<a>` 元素, 那么直接获取该元素的 `href` 属性即可. 
> - 如果是图片链接 `coverUrl`, 那么是一个 `<img>` 元素, 获取其 `src` 属性也可以, 由于该 `<img>` 属性的图片链接还储存到了 `data-src` 属性中, 所以可以通过 `dataset.src` 获取. 
> - 其它的信息都直接通过 `.textContent` 获取其文本即可. 
>
> 最后将所有需要收集到的信息通过 `Object.fromEntries()` 方法转换成对象即可. 

```ts
// 视频信息类型
type VideoInfo = Record<keyof typeof videoInfoSelectorMapper, string>;
```

```ts
/**
 * 解析单独的视频信息
 */
const parseVideoCardInfo = ( videoCard: HTMLElement ) => {
	const videoInfoEntry: [ string, string ][] = Object.entries( videoInfoSelectorMapper )
		.map( ( [ key, selector ] ) => {
			const element = videoCard.querySelector<HTMLElement>( selector );
            // 统一返回 [string, string] 元组的格式, 因为数据是以元组形式获取的, 以该元组的形式返回之后, 可以通过 `Object.fromEntries()` 方法直接转换成对象. 
			if ( !element ) {
				return [ key, '' ];
			}
			if ( key === 'coverUrl' ) {
                // 补上缺少的 https 前缀
				return [ key, `https:${ element.dataset.src }` || '' ];
			}
			if ( key === 'videoUrl' ) {
				return [ key, ( <HTMLAnchorElement> element ).href ];
			}
            // .trim() 过滤掉可能存在的制表符/换行符
			return [ key, element.textContent!.trim() || '' ];
		} );
	return Object.fromEntries( videoInfoEntry ) as VideoInfo;
};
```

**处理每一个视频卡片信息**

> 通过 `videoCardList.map()` 遍历一下, 传进 `parseVideoCardInfo` 处理即可. 

```ts
// 视频卡片信息列表
const videoInfoList: VideoInfo[] = videoCardList.map( parseVideoCardInfo );
```

---

**下载信息**

> 在这个阶段其实怎么处理都可以的, 看你的需求是想要输出什么格式的信息文档了. 接下来我会输出 `json` 和 `csv` 文档举例. 

**获取当前每周必看期数**

输出文档之前, 需要先知道当前下载的文档是属于第几期每周必看的, 还是一样从页面中获取. 

这里是因为网址参数中存在期数计数, 即 `num` 属性, 所以从网址参数中获取会更方便一点. 

通过 `new URLSearchParams()` 构造函数可以直接解析网址参数, 不用手动去处理. 

```ts
// 每周必看期数
const stageNum = new URLSearchParams( location.search ).get( 'num' ) || '0';
```

**输出 json 文档**

> 输出 json 文档是最简单的, 因为我们的数据格式就是 json, 直接通过 `JSON.stringify()` 转换一下就可以. 

```ts
/**
 * 将数据解析成 json 格式
 */
const parseDataToJson = ( data: VideoInfo[] ) => {
	return JSON.stringify( data, null, '\t' );
};
```

```ts
const handleStartDataCollect = () => {
    /* 获取列表容器元素 */
    /* 获取视频信息卡片元素列表 */
    /* 处理每一个视频卡片信息 */
    
    // 获取 json 字符串
    const jsonString = parseDataToJson( videoInfoList );
    
    // 将 json 字符串文本下载到本地
    gmDownload.text(
		jsonString,
		`BilibiliWeeklyRecommend_${ stageNum }.json`,
		'application/json',
	)
		.then( () => {
			GM_notification( `第 ${ stageNum } 期每周必看数据爬取完成` );
		} );
}
```

> 这里的 `gmDownload.text()` 是我个人使用的库 [`@yiero/gmlib`](https://www.npmjs.com/package/@yiero/gmlib) 中的辅助函数, 是一个使用 `Promise` 实现的 `GM_download` , 并且直接传入文本就可以完成下载. 

**输出 csv 文档**

> 输出 csv 文档 (或者 markdown 文档, Excel 文档等) 就相对会麻烦一些了, 因为需要自己手动将信息处理成文本格式. 

```ts
/**
 * 将数据解析成 CSV 格式
 */
const parseDataToCSV = ( data: VideoInfo[] ) => {
	return '视频标题,up主,视频链接,封面链接,推荐语,播放量,弹幕量\n'
		+ data.map( ( item ) => {
			return `${ item.title },${ item.up },${ item.videoUrl },${ item.coverUrl },${ item.recommended },${ item.views },${ item.danmakus }`;
		} ).join( '\n' );
};
```

> 这里下载的 csv 文档直接通过 Excel 打开会乱码, 因为通过 Excel 打开 csv 文档要不乱码查看只能是 ASCII 编码的, 而我们输出的文本是 Unicode 编码的. 
>
> 如果想要输出 ASCII 编码的文件, 还需要将 `csvString` 转换为 ASCII 字节，再传入 `gmDownload.text()` 中

```ts
const handleStartDataCollect = () => {
    /* 获取列表容器元素 */
    /* 获取视频信息卡片元素列表 */
    /* 处理每一个视频卡片信息 */
    
    // 获取 csv 字符串
    const csvString = parseDataToCSV( videoInfoList );
    
    // 将 csv 字符串文本下载到本地
    gmDownload.text(
		csvString,
		`BilibiliWeeklyRecommend_${ stageNum }.csv`,
		'application/csv' )
		.then( () => {
			GM_notification( `第 ${ stageNum } 期每周必看数据爬取完成` );
		} );
}
```

### 完整代码

> - [Bilibili 每周必看数据爬取v1](https://scriptcat.org/zh-CN/script-show-page/3528/code) 
